# -*- coding: utf-8 -*-
"""Bart.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14d6fdcsoUvOViZjprR8J_wyUQBrRqsUa
"""

import json
import torch
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
# Ensure rouge_score is installed
try:
    from rouge_score import rouge_scorer
except ImportError:
    import subprocess, sys
    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'rouge-score'])
    from rouge_score import rouge_scorer
from transformers import BartTokenizer, BartForConditionalGeneration
from tqdm import trange

# === Configuration ===
MODEL_NAME    = 'facebook/bart-large-cnn'
FILE_PATH     = '/content/totto_dev_data.jsonl'
OUTPUT_FILE   = '../../../Downloads/bart_output.txt'
MAX_TABLES    = 1000
BATCH_SIZE    = 8    # adjust to fit memory
MAX_INPUT_LEN = 512
MAX_GEN_LEN   = 40   # shorten output length
MIN_GEN_LEN   = 5    # ensure minimum length
DEVICE        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
USE_FP16      = False  # set True if GPU memory allows

# Initialize BLEU & ROUGE
def get_best_matching_ground_truth(response, truths):
    smooth = SmoothingFunction().method4
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    best_bleu, best_rouge, best_ref = -1.0, -1.0, None
    for ref in truths:
        bleu = sentence_bleu([ref.split()], response.split(), smoothing_function=smooth)
        rouge_l = scorer.score(ref, response)['rougeL'].fmeasure
        if bleu + rouge_l > best_bleu + best_rouge:
            best_bleu, best_rouge, best_ref = bleu, rouge_l, ref
    return best_ref, best_bleu, best_rouge

# Load model and tokenizer
tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)
model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)
model.to(DEVICE)
if DEVICE.type == 'cuda' and USE_FP16:
    model.half()
model.eval()

def generate_batch(prompts):
    inputs = tokenizer(
        prompts,
        return_tensors='pt',
        truncation=True,
        padding=True,
        max_length=MAX_INPUT_LEN
    ).to(DEVICE)
    with torch.no_grad():
        summary_ids = model.generate(
            inputs.input_ids,
            attention_mask=inputs.attention_mask,
            num_beams=5,
            no_repeat_ngram_size=3,
            length_penalty=2.0,
            min_length=MIN_GEN_LEN,
            max_length=MAX_GEN_LEN,
            early_stopping=True,
            eos_token_id=tokenizer.eos_token_id
        )
    return tokenizer.batch_decode(summary_ids, skip_special_tokens=True)

# Load dataset safely
dev_data = []
with open(FILE_PATH, 'r', encoding='utf-8') as f:
    for idx, line in enumerate(f, start=1):
        line = line.strip()
        if not line:
            continue
        try:
            dev_data.append(json.loads(line))
        except json.JSONDecodeError:
            print(f"Skipping malformed JSON on line {idx}")
            continue

max_tables = min(MAX_TABLES, len(dev_data))
torch.cuda.empty_cache()

# Process and evaluate
with open(OUTPUT_FILE, 'w', encoding='utf-8') as out_f:
    for start in trange(0, max_tables, BATCH_SIZE, desc='Tables'):
        batch = dev_data[start:start + BATCH_SIZE]
        prompts = []
        for sample in batch:
            table = sample['table']
            headers = [cell['value'] if isinstance(cell, dict) else str(cell) for cell in table[0]]
            highlights = sample['highlighted_cells']
            pairs = []
            for r, c in highlights:
                cell = table[r][c]
                val = cell['value'] if isinstance(cell, dict) else str(cell)
                hdr = headers[c] if c < len(headers) else ''
                pairs.append(f"{hdr} {val}")
            prompt_text = "Generate a sentence for fields: " + '; '.join(pairs)
            prompts.append(prompt_text)

        responses = generate_batch(prompts)

        for idx, sample in enumerate(batch):
            n = start + idx + 1
            resp = responses[idx].strip()
            truths = [ann['final_sentence'] for ann in sample['sentence_annotations']]
            best_ref, bleu_score, rouge_l = get_best_matching_ground_truth(resp, truths)
            lines = [
                f"Table number {n}",
                f"Highlighted Cells Prompt {sample['highlighted_cells']}",
                f"Table Prompt: {sample['table']}",
                f"Response sentence: {resp}",
                f"Closest Ground Truth: {best_ref}",
                f"BLEU: {bleu_score:.4f}",
                f"ROUGE-L: {rouge_l:.4f}",
                ''
            ]
            for line in lines:
                print(line)
                out_f.write(line + '\n')
            print()
            out_f.write('\n')

print(f"Completed BART processing for {max_tables} tables.")